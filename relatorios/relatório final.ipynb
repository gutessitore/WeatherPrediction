{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"section-0\"></a>\n",
    "<img src=\"https://i.imgur.com/lTCPbz0.jpeg\" alt='logo puc-sp' width='30%' align='left'>\n",
    "\n",
    "### Pontifícia Universidade Católica de São Paulo (PUC-SP)\n",
    "\n",
    "<h1 style=font-size:30px>StellarClassification</h1>\n",
    "\n",
    "\n",
    "### Bacharelado em Ciência de Dados e Inteligência Artificial\n",
    "\n",
    "#### Turma: CDIA21-MA\n",
    "\n",
    "**Professor:** Eduardo Savino Gomes\n",
    "\n",
    "**Alunos:**\n",
    "\n",
    "<table align=\"left\" style=font-size:15px>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th style=\"text-align:left;\">Nome</th>\n",
    "    <th style=\"text-align:left;\">RA</th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Carlos Eduardo de Oliveira</td>\n",
    "    <td>RA00297792</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Gustavo Schlieper Tessitore</td>\n",
    "    <td>RA00297844</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Isaac Higuchi</td>\n",
    "    <td>RA00306191</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">João Pedro Taves Araujo</td>\n",
    "    <td>RA00297753</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Lucas Lopes Amorim</td>\n",
    "    <td>RA00303799</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Thiago de Jesus Carvalho</td>\n",
    "    <td>RA00297767</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3><b>Estrutura:</b></h3>\n",
    "<ol>\n",
    "   <li>\n",
    "      <a href=\"#section-1\">Objetivos</a>\n",
    "      <ol style=\"list-style: none; padding-left: 10px\">\n",
    "         <li>1.1. <a href=\"#section-1-1\">Objetivos Gerais</a></li>\n",
    "         <li>1.2. <a href=\"#section-1-2\">Objetivos Específicos</a></li>\n",
    "      </ol>\n",
    "   </li>\n",
    "   <li><a href=\"#section-2\">Cronograma & Tarefas</a></li>\n",
    "   <li><a href=\"#section-3\">Recursos Necessários</a></li>\n",
    "   <li><a href=\"#section-4\">Critérios de avaliação </a></li>\n",
    "   <li><a href=\"#section-5\">Resultados</a>\n",
    "      <ol style=\"list-style: none; padding-left: 10px\">\n",
    "         <li>5.1. <a href=\"#section-5-1\">API's consumidas</a></li>\n",
    "         <li>5.2. <a href=\"#section-5-2\">Banco de Dados</a></li>\n",
    "         <li>5.3. <a href=\"#section-5-3\">Técnicas de IA e ML Utilizadas</a></li>\n",
    "         <li>5.4. <a href=\"#section-5-4\">API para Consumo dos Dados</a></li>\n",
    "         <li>5.5. <a href=\"#section-5-5\">Dashboards</a></li>\n",
    "      </ol>\n",
    "   </li>\n",
    "</ol>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Objetivos <a class=\"anchor\" id=\"section-1\"></a>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Objetivos Gerais <a class=\"anchor\" id=\"section-1-1\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "O objetivo geral do projeto Traffic and Energy Forecast é desenvolver dois produtos de inteligência artificial:\n",
    "\n",
    "- Um aplicativo que prevê a energia gerada na instalação de painéis solares com base em alguns inputs do usuário\n",
    "- Um aplicativo que prevê o congestionamento do trânsito de São Paulo com base na série histórica"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Objetivos Específicos <a class=\"anchor\" id=\"section-1-2\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "- Consumir as APIs necessárias e obter os dados necessários para a previsão de engarrafamentos e energia gerada;\n",
    "- Processar os dados obtidos aplicando filtros e totalizadores;\n",
    "- Armazenar os dados filtrados em um banco de dados SQL;\n",
    "- Desenvolver um dashboard para apresentar os dados de forma clara e intuitiva;\n",
    "- Aplicar técnicas de IA, como o Keras TensorFlow, para treinar modelos de machine learning e fazer previsões precisas de engarrafamentos e energia gerada;\n",
    "- Disponibilizar os modelos de machine learning em uma API para consumo externo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Cronograma & Tarefas <a class=\"anchor\" id=\"section-2\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "As atividades necessárias para a realização do projeto Traffic and Energy Forecast são:\n",
    "<br>\n",
    "\n",
    "#### Semana 1\n",
    "- Definir a categoria de interesse da equipe e pesquisar hubs de API que possam fornecer os dados mais relevantes para a categoria escolhida;\n",
    "- Escolher as API’s mais adequada e definir os objetivos gerais e específicos do projeto;\n",
    "- Estabelecer o cronograma de atividades e prazos de entrega;\n",
    "- Identificar os recursos necessários, como hardware, software e habilidades.\n",
    "\n",
    "#### Semana 2\n",
    "- Realizar a integração com as API’s e consumir os dados necessários;\n",
    "- Realizar o processamento dos dados, aplicando filtros e totalizadores, e armazená-los em memória;\n",
    "- Identificar e escolher o banco de dados SQL mais adequado para armazenar os dados filtrados;\n",
    "- Começar a desenvolver os dashboard’s escolhendo a ferramenta mais adequada.\n",
    "\n",
    "#### Semana 3\n",
    "- Finalizar o desenvolvimento dos dashboard’s, garantindo uma boa apresentação dos dados;\n",
    "- Treinar o modelo de machine learning utilizando o Keras tensorflow e aplicar técnicas de IA para fazer predição de dados;\n",
    "- Treinar o modelo de machine learning utilizando o autogluon e aplicar técnicas de IA para fazer predição de dados;\n",
    "- Desenvolver as API’s para consumir os dados tratados e totalizados e disponibilizar o modelo de machine learning.\n",
    "\n",
    "#### Semana 4\n",
    "- Realizar testes nas API’s e no modelo de machine learning para garantir a qualidade do produto final;\n",
    "- Revisar e refinar o plano de acordo com feedback recebido e realizar correções necessárias;\n",
    "- Preparar e apresentar o projeto para avaliação final, de acordo com os critérios de avaliação definidos previamente.\n",
    "- Há dependências entre as tarefas, pois a equipe deve primeiro consumir as API’s de trânsito e irradiação para processar os dados antes de armazená-los em um Banco de Dados SQL. Além disso, o desenvolvimento do Dashboard depende dos dados processados e armazenados, e o treinamento do modelo de machine learning depende dos dados disponíveis no Banco de Dados SQL."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tarefas\n",
    "\n",
    "As tarefas serão divididas entre os membros da equipe da seguinte forma:\n",
    "\n",
    "\n",
    "<table align=\"left\" style=font-size:15px>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th style=\"text-align:left;\">Tarefa</th>\n",
    "    <th style=\"text-align:left;\">Membro(s) responsável(eis) </th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Pesquisa e identificação de APIs para coleta de dados de trânsito e irradiação </td>\n",
    "    <td>Todos</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Consumo das API’s citadas acima e processamento dos dados </td>\n",
    "    <td>Gustavo, JP, Cadu, Isaac </td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Armazenamento dos dados filtrados em um Banco de Dados BigQuery </td>\n",
    "    <td>Gustavo</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Desenvolvimento dos Dashboard’s </td>\n",
    "    <td>Lucas, Cadu (Energia), Isaac, Thiago (Engarafamento) </td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Treinamento do modelo de machine learning e predições  (energia gerada por painéis solares e trânsito (km))  </td>\n",
    "    <td>João Pedro, Cadu, Lucas </td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left;\">Desenvolvimento das API’s para consumir os dados tratados </td>\n",
    "    <td>Gustavo</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Tarefas do Projeto | Semana 1 | Semana 2 | Semana 3 | Semana 4 |\n",
    "| :-- | :-: | :-: | :-: | :-: |\n",
    "| Pesquisa e identificação das API’s  | X |  |  |  |\n",
    "| Consumo das API’s e processamento dos dados |  | X |  |  |\n",
    "| Armazenamento dos dados filtrados em um Banco de Dados SQL |  |  | X |  |\n",
    "| Desenvolvimento dos Dashboards |  | X | X |  |\n",
    "| Treinamento do modelo de machine learning e predição de energia gerada por painéis solares e trânsito (km)  |  |  | X | X |\n",
    "| Desenvolvimento das API’s para consumir os dados tratados e totalizados |  |  | X | X |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Recursos Necessários <a class=\"anchor\" id=\"section-3\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "- Acesso a pelo menos uma API com dados de irradiação ou de congestionamento de trânsito;\n",
    "- Linguagem de programação Python;\n",
    "- Bibliotecas Python, como requests, pandas, Keras tensorflow, Autogluon e Streamlit;\n",
    "- Banco de Dados SQL;\n",
    "- Recursos de hardware, como computadores com capacidade suficiente para processar os dados e treinar o modelo de machine learning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Critérios de avaliação <a class=\"anchor\" id=\"section-4\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "\n",
    "Para avaliar se o cronograma está sendo executado de acordo com o planejado, a equipe deve monitorar o andamento das tarefas semanalmente e comparar com o cronograma estabelecido. Para garantir que o produto final atenda às expectativas, serão definidos os seguintes critérios de avaliação:\n",
    "\n",
    "- Precisão das previsões do clima: o modelo de machine learning deve ser capaz de fazer previsões precisas da geração de energia e congestionamento com base nos dados obtidos das API’s de trânsito e irradiação;\n",
    "- Qualidade do Dashboard: o dashboard deve apresentar os dados de forma clara e intuitiva, permitindo que os usuários visualizem as informações relevantes de maneira fácil e rápida;\n",
    "- Qualidade das API’s: as API’s devem fornecer os dados tratados e totalizados de forma confiável e eficiente;\n",
    "- Integração com o Banco de Dados SQL: os dados filtrados devem ser armazenados de maneira eficiente e acessíveis para o treinamento do modelo de machine learning e consumo pela API;\n",
    "- Atendimento aos requisitos: o produto final deve atender aos requisitos estabelecidos pela equipe e pelo cliente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Resultados <a class=\"anchor\" id=\"section-5\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "## 5.1 API's consumidas <a class=\"anchor\" id=\"section-5-1\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "Utilizamos a API openwaether para coletar os dados climáticos, OpenMeteo para dados de irradiação solar e API do IBGE para converter as coordenadas geográficas\n",
    "\n",
    "\n",
    "## 5.2 Banco de Dados <a class=\"anchor\" id=\"section-5-2\"></a>\n",
    "[Voltar ao topo](#section-1)<br>\n",
    "Utilizamos o BigQuery para armazenar os dados consumidos das API's. Para ter acesso ao banco de dados, entre em contato com os alunos para disponibilizar o arquivo de credenciais.\n",
    "\n",
    "Tambem fizemos uma API para consumir os dados do BQ, com o código abaixo é possivel consumir os dados:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"query\": \"SELECT * FROM `your_project.your_dataset.your_table` LIMIT 10;\"}' http://127.0.0.1:5000/query\n",
    "\"\"\"\n",
    "from flask import Flask, jsonify, request\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configuração da autenticação\n",
    "# Substitua 'path/to/keyfile.json' pelo caminho do arquivo de chave JSON da sua conta de serviço do Google Cloud\n",
    "\n",
    "api_key_json_path = r\"C:\\Users\\gugat\\Documents\\puc-sp-ab9bc14a5151.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = api_key_json_path\n",
    "bigquery_client = bigquery.Client.from_service_account_json(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])\n",
    "\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def run_query():\n",
    "    data = request.get_json()\n",
    "    query = data.get('query')\n",
    "\n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"A chave 'query' é obrigatória no JSON.\"}), 400\n",
    "\n",
    "    try:\n",
    "        query_job = bigquery_client.query(query)\n",
    "        results = query_job.result()\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "    rows = [dict(row) for row in results]\n",
    "    return jsonify({\"data\": rows}), 200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "![image.png](attachment:image.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Técnicas de IA e ML Utilizadas <a class=\"anchor\" id=\"section-5-3\"></a>\n",
    "[Voltar ao topo](#section-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 API para Consumo dos Dados <a class=\"anchor\" id=\"section-5-4\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "Para consumir os dados, basta utilizar o código abaixo:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"feature1\": 1, \"feature2\": 2, \"feature3\": 3, \"feature4\": 4}' http://127.0.0.1:5000/predict/transito\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Carrega a configuração das features\n",
    "config = {\n",
    "  \"modelo_transito\": {\n",
    "      \"features\": [],\n",
    "      \"input_data_structure\": \"np.reshape(np.array({0}), (1, np.array({0}).shape[0], np.array({0}).shape[1]))\",\n",
    "      \"load_function\": \"load_model\",\n",
    "      \"file_extension\": \"h5\"\n",
    "  },\n",
    "  \"modelo_irradiacao\": {\n",
    "      \"features\": [\n",
    "        \"year\", \"month\", \"temp ºC\", \"feels_like ºC\", \"pressure\", \"humidity %\", \"clouds %\", \"visibility\", \"wind_speed m/s\"\n",
    "      ],\n",
    "      \"input_data_structure\": \"pd.DataFrame({0}, index=[0])\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "def _load_model(model_name, load_func=joblib.load, file_extension='pkl'):\n",
    "    model_path = os.path.join('..', 'models', f'{model_name}.{file_extension}')\n",
    "    if not os.path.isfile(model_path):\n",
    "        return None\n",
    "    return load_func(model_path)\n",
    "\n",
    "\n",
    "@app.route('/predict/transito', methods=['POST'])\n",
    "def predict_transito():\n",
    "    return predict('modelo_transito')\n",
    "\n",
    "\n",
    "@app.route('/predict/irradiacao', methods=['POST'])\n",
    "def predict_irradiacao():\n",
    "    return predict('modelo_irradiacao')\n",
    "\n",
    "\n",
    "def predict(model_name):\n",
    "    data = request.get_json()[0]\n",
    "    features = config[model_name]['features']\n",
    "\n",
    "    missing_features = [k for k in features if k not in data]\n",
    "    if missing_features:\n",
    "        return jsonify(\n",
    "            {\"error\": \"Dados insuficientes para realizar a previsão.\", \"missing_features\": missing_features}), 400\n",
    "\n",
    "    # Carregas o modelos\n",
    "    for model_name in config.keys():\n",
    "        load_function = eval(config[model_name].get('load_function', \"joblib.load\"))\n",
    "        file_extension = config[model_name].get('file_extension', 'pkl')\n",
    "        model = _load_model(model_name, load_function, file_extension)\n",
    "        if model is None:\n",
    "            return jsonify({\"error\": f\"Modelo '{model_name}' não encontrado.\"}), 404\n",
    "\n",
    "    # Extrai os dados das features\n",
    "    feature_data = eval(config[model_name]['input_data_structure'].format(\"data\"))\n",
    "\n",
    "    # Realiza a previsão\n",
    "    prediction = model.predict(feature_data)[0]\n",
    "\n",
    "    # Retorna o resultado\n",
    "    return jsonify({f\"{model_name}_prediction\": float(prediction)}), 200\n",
    "\n",
    "\n",
    "def run():\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "```\n",
    "\n",
    "Tambem deixamos exemplos das chamadas de API's nos respectivos notebooks de cada modelo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 Dashboards <a class=\"anchor\" id=\"section-5-5\"></a>\n",
    "[Voltar ao topo](#section-1)\n",
    "\n",
    "## Resultados Dash Energia\n",
    "API utilizada: https://open-meteo.com/en/docs/historical-weather-api\n",
    "<br>\n",
    "Para prever a quantidade de energia gerada em painel(éis) solar(es), nossa API coleta dados históricos de irradiação de uma determinada localização inserida pelo usuário (latitude e longitude) e utiliza esses dados para o treinamento de uma Série Temporal. Assim, com base nos resultados do modelo de ML, é feito o cálculo automatizado da quantidade de energia gerada e o retorno de investimento em reais.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resultados Dash e modelos Congestionamento\n",
    "\n",
    "Para prever a quantidade de tráfego, fizemos alguns “literature reviews” para nos informar sobre as técnicas utilizadas em tecnologias estado da arte que atacam o mesmo problema. Após as pesquisas, chegamos à conclusão de que a melhor forma de atacar o problema da nossa perspectiva seria coletar dados da CET (Companhia de Engenharia de Tráfego) de SP para cruzarmos com os dados de clima de forma a basearmos nossas previsões de tráfego. Em relação à arquitetura do modelo em si, utilizamos modelos de Deep Learning para previsão de séries temporais, dada a natureza temporal dos dados.\n",
    "Após a análise dos modelos, observamos performance satisfatória na aproximação dos valores reais pelos preditos nos dados de teste. No entanto, como modelos de Deep Learning requerem um volume maior de dados para diminuir o overfitting e aumentar a performance, consideramos que deveríamos em versões futuras aumentar o volume de dados de treino para que possamos nos aproximar mais ainda de modelos estado da arte, os quais preveem nossa variável-alvo com erros na ordem de até 10-6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}